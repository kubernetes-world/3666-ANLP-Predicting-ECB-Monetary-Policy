{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWJTYWx7Ibd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# RATE_SPEECHES_FILE = \"joined.parquet.gzip\"\n",
        "# assert os.path.exists(RATE_SPEECHES_FILE), f\"file not present: {RATE_SPEECHES_FILE}\"\n",
        "\n",
        "# rate_speeches = pd.read_parquet(RATE_SPEECHES_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "id": "gTmNryQKqIj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "\n",
        "# def rate_speeches_text_length():\n",
        "#   '''\n",
        "#   reload rate_speeches or update it with text lengths\n",
        "#   '''\n",
        "#   global rate_speeches\n",
        "#   RATE_SPEECHES_TL_FILE = 'rate_speeches.text_length.parquet.gzip'\n",
        "\n",
        "#   if os.path.exists(RATE_SPEECHES_TL_FILE):\n",
        "#     print(f\"loading {RATE_SPEECHES_TL_FILE}...\")\n",
        "#     rate_speeches = pd.read_parquet(RATE_SPEECHES_TL_FILE)\n",
        "#   else:\n",
        "#     rate_speeches['text_length'] = rate_speeches['extracted_text'].apply(lambda x: len(word_tokenize(x)))\n",
        "#     rate_speeches.to_parquet(RATE_SPEECHES_TL_FILE, compression='gzip')\n",
        "\n",
        "# rate_speeches_text_length();\n",
        "\n",
        "# print(rate_speeches['text_length'].describe())\n",
        "# rate_speeches['extracted_text'][0]\n"
      ],
      "metadata": {
        "id": "6wbWJvAFpIx_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rate_speeches_sent_tokenize():\n",
        "  '''\n",
        "  reload rate_speeches or update it with `sent_tokenize`\n",
        "  '''\n",
        "  global rate_speeches\n",
        "  RATE_SPEECHES_TOKENIZED_FILE = 'rate_speeches.sent_tokenize.parquet.gzip'\n",
        "\n",
        "  if os.path.exists(RATE_SPEECHES_TOKENIZED_FILE):\n",
        "    print(f\"loading {RATE_SPEECHES_TOKENIZED_FILE}...\")\n",
        "    rate_speeches = pd.read_parquet(RATE_SPEECHES_TOKENIZED_FILE)\n",
        "  else:\n",
        "    rate_speeches['extracted_text'] = rate_speeches['extracted_text'].apply(sent_tokenize)\n",
        "    rate_speeches.to_parquet(RATE_SPEECHES_TOKENIZED_FILE, compression='gzip')\n",
        "\n",
        "rate_speeches_sent_tokenize()\n"
      ],
      "metadata": {
        "id": "dM5UZIR5oNHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "from itertools import groupby\n",
        "from unicodedata import category as unicat\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.chunk import tree2conlltags\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.chunk.regexp import RegexpParser\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extract adverbial and adjective phrases, and transform\n",
        "    documents into lists of these keyphrases, with a total\n",
        "    keyphrase lexicon limited by the nfeatures parameter\n",
        "    and a document length limited/padded to doclen\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeatures: int, doclen: int):\n",
        "        self.grammar = r'KT: {(<RB.> <JJ.*>|<VB.*>|<RB.*>)|(<JJ> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {(<RB.*> <VB.>|<RB.>|<JJ.> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {<RB.>|<JJ.>}'\n",
        "        self.chunker = RegexpParser(self.grammar)\n",
        "        self.nfeatures = nfeatures\n",
        "        self.doclen = doclen\n",
        "        self._curr = 0\n",
        "\n",
        "    def normalize(self, sent):\n",
        "        \"\"\"\n",
        "        Removes punctuation from a tokenized/tagged sentence and\n",
        "        lowercases words.\n",
        "        \"\"\"\n",
        "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
        "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
        "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
        "        return list(sent)\n",
        "\n",
        "    def extract_candidate_phrases(self, sents, call_no):\n",
        "        \"\"\"\n",
        "        For a document, parse sentences using our chunker created by\n",
        "        our grammar, converting the parse tree into a tagged sequence.\n",
        "        Extract phrases, rejoin with a space, and yield the document\n",
        "        represented as a list of it's keyphrases.\n",
        "        \"\"\"\n",
        "        print(f\">> \\t [{call_no}] extract_candidate_phrases...\")\n",
        "\n",
        "        for i, sent in enumerate(sents):\n",
        "            tokens = word_tokenize(sent)\n",
        "            pos_tags = pos_tag(tokens)\n",
        "            normalized = self.normalize(pos_tags)\n",
        "            # print(f\"[{self._curr}] sent #{i}: {sent}\")\n",
        "            # print(f\"[{self._curr}] \\t {normalized}\")\n",
        "\n",
        "            chunks = tree2conlltags(self.chunker.parse(normalized))\n",
        "            if not chunks or all(chunk[-1] == 'O' for chunk in chunks):\n",
        "                #print(f\"No valid chunks found in sentence: {sent}\")\n",
        "                continue\n",
        "\n",
        "            phrases = [\n",
        "                \" \".join(word for word, pos, chunk in group).lower()\n",
        "                for key, group in groupby(\n",
        "                    chunks, lambda term: term[-1] != 'O'\n",
        "                ) if key\n",
        "            ]\n",
        "            for phrase in phrases:\n",
        "                yield phrase\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_lexicon(self, keydocs):\n",
        "        \"\"\"\n",
        "        Build a lexicon of size nfeatures\n",
        "        \"\"\"\n",
        "        keyphrases = [keyphrase for doc in keydocs for keyphrase in doc]\n",
        "        print(\"Keyphrases:\", keyphrases[:5])\n",
        "        fdist = FreqDist(keyphrases)\n",
        "        counts = fdist.most_common(self.nfeatures)\n",
        "        print(\"Frequency counts:\", counts[:5])\n",
        "        lexicon = [phrase for phrase, count in counts]\n",
        "        return {phrase: idx+1 for idx, phrase in enumerate(lexicon)}\n",
        "\n",
        "    def clip(self, keydoc, lexicon):\n",
        "        \"\"\"\n",
        "        Remove keyphrases from documents that aren't in the lexicon\n",
        "        \"\"\"\n",
        "        return [lexicon[keyphrase] for keyphrase in keydoc\n",
        "            if keyphrase in lexicon.keys()]\n",
        "\n",
        "    def transform(self, documents):\n",
        "      self._curr += 1\n",
        "      print(f\">> [{self._curr}] KeyphraseExtractor.transform: {len(documents)}...\")\n",
        "      docs = [list(self.extract_candidate_phrases(doc, i)) for i, doc in enumerate(documents)]\n",
        "      lexicon = self.get_lexicon(docs)\n",
        "      clipped = [list(self.clip(doc, lexicon)) for doc in docs]\n",
        "      return pad_sequences(clipped, maxlen=self.doclen)"
      ],
      "metadata": {
        "id": "T0tWYGJmIAIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def data_set(view):\n",
        "  X = view['extracted_text']\n",
        "  display(X.describe())\n",
        "\n",
        "  # view = rate_speeches[:doclen]\n",
        "  labels = view[\"Direction\"].values\n",
        "  label_mapping = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
        "  encoded_labels = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "  # convert to one-hot encoding\n",
        "  num_classes = len(label_mapping)\n",
        "  print(f\"num_classes: {num_classes}\")\n",
        "  y = to_categorical(encoded_labels, num_classes=num_classes)\n",
        "  return X, y, num_classes\n"
      ],
      "metadata": {
        "id": "YHlL7BP1I6Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qJ7sWmqdhvqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense, Input\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def create_lstm_model(vocab_size: int, input_length: int, num_classes: int):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_length,), name=\"input_layer\"),  # explicit input layer\n",
        "        Embedding(input_dim=vocab_size, output_dim=128, name=\"embedding_layer\"),\n",
        "        LSTM(128, return_sequences=True, name=\"lstm_layer_1\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_1\"),\n",
        "        LSTM(64, name=\"lstm_layer_2\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_2\"),\n",
        "        Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ghUGperddEGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "3s_4cY_xi1ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# X, y, num_classes = data_set(rate_speeches[:60])\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# keyphrase_extractor = KeyphraseExtractor(nfeatures=10000, doclen=100)\n",
        "# X_train = keyphrase_extractor.fit_transform(X_train)\n",
        "# X_val = keyphrase_extractor.transform(X_val)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v4_weIC7jd_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = create_lstm_model(vocab_size=10000, input_length=100, num_classes=num_classes);\n",
        "\n",
        "# history = model.fit(\n",
        "#     X_train, y_train,\n",
        "#     validation_data=(X_val, y_val),\n",
        "#     epochs=10,\n",
        "#     batch_size=32\n",
        "# )\n"
      ],
      "metadata": {
        "id": "mTIihNKi44rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loss, accuracy = model.evaluate(X_val, y_val)\n",
        "# print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "LeZ20D4OkxVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DebugTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"Data Shape Before LSTM:\", X.shape)\n",
        "        # print(\"Sample Data Before LSTM:\", X[0])  # print a sample\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "Z9Oezl_Klq9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from joblib import Memory\n",
        "\n",
        "X, y, num_classes = data_set(rate_speeches)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0P8zESR_iKXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyphrase_extractor = KeyphraseExtractor(nfeatures=10000, doclen=100)\n",
        "X_transformed = keyphrase_extractor.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "gCvM0-c2LJyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"X_transformed.npy\", X_transformed)\n"
      ],
      "metadata": {
        "id": "YEYUWVvgN241"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed = np.load(\"X_transformed.npy\")\n"
      ],
      "metadata": {
        "id": "g_gkdB1xMkBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set up caching directory\n",
        "memory = Memory(location=\"cache_directory\", verbose=0)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # (\"keyphrase_extractor\", # don't do this!, pre-compute this step instead!!!\n",
        "    #   KeyphraseExtractor(nfeatures=10000, doclen=100)),\n",
        "    ('debug', DebugTransformer()),\n",
        "    (\"lstm_classifier\",\n",
        "      KerasClassifier(\n",
        "        build_fn=create_lstm_model,\n",
        "        vocab_size=10000,\n",
        "        input_length=100,\n",
        "        num_classes=num_classes,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=1)\n",
        "    )\n",
        "], memory=memory)\n"
      ],
      "metadata": {
        "id": "Fsq4YdbqDWHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = cross_val_score(pipeline, X_transformed, y, cv=4, scoring=\"accuracy\")\n"
      ],
      "metadata": {
        "id": "Ht894d-0ESx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cross-validation scores: {scores}\")\n",
        "print(f\"Mean accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "tVVt3XcdJC_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}