{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lmWJTYWx7Ibd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "RATE_SPEECHES_FILE = \"joined.parquet.gzip\"\n",
        "assert os.path.exists(RATE_SPEECHES_FILE), f\"file not present: {RATE_SPEECHES_FILE}\"\n",
        "\n",
        "rate_speeches = pd.read_parquet(RATE_SPEECHES_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTmNryQKqIj6",
        "outputId": "46ec8cfa-57e5-4c52-8f77-e0a26049cf0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def rate_speeches_text_length():\n",
        "  '''\n",
        "  reload rate_speeches or update it with text lengths\n",
        "  '''\n",
        "  global rate_speeches\n",
        "  RATE_SPEECHES_TL_FILE = 'rate_speeches.text_length.parquet.gzip'\n",
        "\n",
        "  if os.path.exists(RATE_SPEECHES_TL_FILE):\n",
        "    print(f\"loading {RATE_SPEECHES_TL_FILE}...\")\n",
        "    rate_speeches = pd.read_parquet(RATE_SPEECHES_TL_FILE)\n",
        "  else:\n",
        "    rate_speeches['text_length'] = rate_speeches['extracted_text'].apply(lambda x: len(word_tokenize(x)))\n",
        "    rate_speeches.to_parquet(RATE_SPEECHES_TL_FILE, compression='gzip')\n",
        "\n",
        "rate_speeches_text_length();\n",
        "\n",
        "print(rate_speeches['text_length'].describe())\n",
        "rate_speeches['extracted_text'][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "6wbWJvAFpIx_",
        "outputId": "3dce76ec-b337-4ceb-dea2-f593a3baec01",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading rate_speeches.text_length.parquet.gzip...\n",
            "count     3434.000000\n",
            "mean      3307.266453\n",
            "std       2296.389089\n",
            "min          0.000000\n",
            "25%       1809.000000\n",
            "50%       2753.000000\n",
            "75%       4115.750000\n",
            "max      26091.000000\n",
            "Name: text_length, dtype: float64\n",
            "CPU times: user 478 ms, sys: 258 ms, total: 736 ms\n",
            "Wall time: 678 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Willem F. Duisenberg, President of the European Central Bank,Thursday, 4 March 1999Ladies and gentlemen, the Vice-President and I are here today to report on the outcome of today\\'s meetings of the Governing Council and of the General Council of the European Central Bank.Let me start with the Governing Council\\'s discussion onrecent economic developments and the decisions that the Governing Council has taken today in the field of monetary policy.After a comprehensive and careful examination of recent trends and ongoing evaluations of the economic outlook for the euro area economy, there was consensus that some of the risks identified earlier, in particular with regard to real GDP growth, had materialised in the fourth quarter of 1998. There was also consensus that the impact of these developments on the balance of risks for price stability would need to be examined further in the context of the monetary policy strategy adopted by the Eurosystem. At this juncture, taking into account all the information available, the Governing Council confirmed its earlier assessment of the outlook for price stability. Against this background it decided that for the main refinancing operations to be settled on 10 and 17 March 1999 the same conditions will apply as for the previous one, i.e. they will be fixed rate tenders conducted at an interest rate of 3.0%. In addition, the interest rate on the marginal lending facility will continue to be 4.5% and the interest rate on the deposit facility will remain 2.0%.Let me report in some more detail on the Governing Council\\'s deliberations, and thereby provideexplanationsfor the decisions we took today.Starting with themonetary developmentsin the euro area at the start of Stage Three of EMU, the broad monetary aggregate M3 expanded at an annual rate of 5.7% in January 1999, against 4.5% in December 1998. This upturn was largely due to an acceleration in overnight deposits. This, in turn, may be explained by the low level of interest rates, which also contributed to credit to the private sector to accelerate further in January 1999 to 9,4% which is particularly notable as other indicators of the economy are signalling a slowdown in activity.Monthly data on monetary aggregates can be volatile. M3 growth is therefore monitored on the basis of three-month moving averages of the 12-month growth rate of M3, which stood at 4.9% in the period from November 1998 to January 1999. This was 0.2 percentage point higher than the figure recorded in the previous three-month period and is 0.4 percentage point above the reference value of 4½%. Against this background and in view of the uncertainty relating to special factors pertaining to the changeover to the Stage Three environment and the introduction of the euro, the Governing Council does not consider the acceleration of M3 at the start of Stage Three as a signal of future inflationary pressures. However, a close monitoring of monetary developments in the coming months remain necessary to give more conclusive evidence of the underlying causes and the permanent or temporary nature of the rise in M3 growth.Turning to financial indicators, euro area capital markets underwent a correction in February, which was mainly linked to spillovers from the United States and Japan. This followed a decline in long-term interest rates in January which had brought bond yields in the euro area to the lowest levels seen since the late 1940s. The euro area yield curve shifted upwards across the entire maturity spectrum during February. Let me also recall that the nominal effective exchange rate of the euro area in early March stood around 1% below its level on 1 February and around 4% below its starting level on 4 January 1999.At the same time, recent developments appear to have led to a reduction in some of the uncertainties surrounding the evolution of theworld economyin 1999. This mainly relates to the strong performance of the US economy, but may also be due to signs of improvement in the real economies of some Asian. Nevertheless, in the view of the Governing Council, it is too early at present for a truly favourable assessment of the external environment for the euro area. Risks pertaining to external imbalances, continuing risk aversion and the corresponding weakness of private long-term capital flows and real investment to emerging market economies will need to be examined closely.In theeuro areathe latest indicators confirm that those risks which were identified earlier have now materialised to some extent. According to preliminary and incomplete data from national sources,real GDP growthin the euro area weakened in the fourth quarter of 1998 when compared with the previous quarter. While more precise estimates will soon become available from Eurostat, the evidence available suggests that activity in the euro area has been slowing down in recent months. This is particularly apparent in theproduction of themanufacturing sector, where output in the fourth quarter of 1998 fell by almost 1% compared with the previous quarter. These developments broadly confirm the pattern observed earlier inindustrial confidence, which was on a declining trend in the second half of 1998. The latest data on industrial confidence in the euro area, i.e. those released by the European Commission today, indicate that industrial confidence has weakened further. Some recent wage developments in the euro area might have contributed to that. The level ofunemploymenthas remained more or less unchanged over the past four months. Finally, consumer confidence in the euro area has remained unchanged from the high levels seen at the beginning of the year. With regard tocapacity utilisationin the manufacturing sector, which fell further in the fourth quarter of 1998, andretail sales, which up to November 1998 pointed towards broadly sustained growth, no new data are yet available.Overall, with regard to the cyclical situation, recent data confirm our earlier expectations that there are still downside risks for output growth. The Governing Council will continue its thorough analysis and very close monitoring of underlying trends and review its broad outlook for the euro area accordingly on an ongoing basis, taking into account in a forward-looking manner the impact of both domestic and external developments.With regard to the latest data on theHarmonised Index of Consumer Prices (HICP),the annual increase for January 1999 was 0.8%, which is unchanged compared with the previous two months (taking account of the most recent revisions). Looking at a breakdown of the HICP, a slight decline in the rate of increase in service prices (from 1.9% in December 1998 to 1.8% in January 1999) was offset by a slight increase in goods prices (from 0.1% to 0.2%). The fall in the rate of change in services prices was mainly due to lower price increases for transport and communications; this may have been a reflection of increased competition in these sectors. On the other hand, downward pressure from energy prices on goods prices diminished somewhat.On balance, the current data continue to suggest that there are no significant upward or downward pressures on prices in the short term. The pattern ofrisks to price stabilityhas also remained broadly unchanged on balance. On the upward side, wage developments are a matter of concern, as would be any loosening of fiscal policies and the implications of this for achieving the objectives laid down in the Stability and Growth Pact. In addition, recent exchange rate trends need to be monitored closely also in terms of their impact on prices. Concerning downward risks, in particular the slowdown in the euro area economy is a cause for concern. Inconclusion, taking the latest available evidence into account, at today\\'s meeting the Governing Council confirmed its assessment regarding the outlook for price developments. The Governing Council therefore decided to keep ECB interest rates unchanged at the levels currently prevailing.I should now like to inform you about someother mattersconsidered today.Theauction technique for the longer-term refinancing operationswas reviewed by the Governing Council today.In its first two longer-term refinancing operations (settled on 14 January and 25 February 1999), which are as a rule conducted by means of variable rate tenders, the Eurosystem applied the single rate (\"Dutch\") method of allotment. The single rate method was chosen in order to encourage less experienced counterparties to participate in the tender. The Governing Council takes the view that all interested counterparties should by now be sufficiently accustomed to the longer-term refinancing operation also to be in a position to participate in this type of operation under the more market-oriented multiple rate (\"American\") method of allotment. Against this background, the Governing Council has decided that the multiple rate method of allotment will be applied starting from the next longer-term refinancing operation (to be settled on 25 March 1999), until otherwise indicated.Finally, let me inform you that the General Council today discussed themonetary policies of the central banks of the non-participating Member Statesagainst the background of the monetary policy set by the Eurosystem. As you will know, the General Council has taken over those tasks of the European Monetary Institute (EMI) which still have to be performed during Stage Three because of the derogations of some Member States. One of these tasks relates to strengthening the co-ordination of the monetary policies in the Community with the aim of ensuring price stability. The General Council intends to conduct these co-ordination exercises biannually in future.We are now at your disposal should you have any questions. Transcript of the questions asked and the answers given by Dr. Willem F. Duisenberg, President of the ECB, and Christian Noyer, Vice-President of the ECBQuestion (translation): Mr. President, what, in your view, are the reasons for the weak euro? Is it due to the interest rate and business activity differentials vis-à-vis the United States, as is argued by some, or is it rather due to the political pressure brought to bear on the central bank, as argue others?Duisenberg:We think it is predominantly the strength of the US dollar, rather than the weakness of the euro, which is determining the picture. And, of course, the interest rate differentials may also have something to do with that. We think those are the main factors explaining the recent decline in the value of the euro, which - if you look at the level which the euro has now reached - I would like to add is more or less the level which the D-Mark had during most of last year, and the strength with which the euro started life was more the exception than the rule.Question:It did not come as a surprise that you did not change interest rates today since you and other Board Members have sort of said so in the last days. You have been pretty clear in what you and Mr. Noyer said. Was that OK with the other Council Members, did you check that with them?Duisenberg:Yeah, we checked it today with them, and it was OK.Question (translation):A young currency requires political support. With regard to the interest rate differential, could part of the weakness be due to the fact that confidence has not yet been underscored politically to the extent needed?Duisenberg:Yes, we are doing our utmost to underscore the confidence that the world has in this young form of money, the euro, and we would certainly hope that not only the ECB, but also politics would help us to the maximum extent possible in supporting this confidence-building process.Question (translation):Is the interest rate differential between the United States and Europe alone not an indication that calls for a reduction of official interest rates in Europe are simply wrong?Duisenberg:I would not place it in that context. But what we are investigating is that there could be another related matter. Namely that the very strong increase in credit to the private sector has partly led or has been used to buy US dollar-denominated assets, attracted by the higher interest rates to be earned in the United States. And that by itself could have, let\\'s say, financed the weakening of the euro or the strength of the dollar. But we are not yet quite sure about that. We will have to have more data available on capital flows and on where the credit is going to precisely, before we can come to any conclusions. But I am happy that you underscore the fact that interest rates in Europe are at an extremely low level, not only historically, but also compared with other major areas in the world. That, in itself, underscores that in the current cyclical situation monetary policy can definitely be regarded as having taken an accommodating stance.Question:Are you concerned that the fall in the value of the euro against the pound sterling could create dangers of inflation in some peripheral countries, such as Ireland, that have a very large trade exposure to sterling?Duisenberg:We do not as yet have any indication whatsoever of that happening. Of course, as I said in my introduction, we will closely monitor, and keep monitoring, the exchange rate developments, also with a view to their impact on, or the impact they might have on prices, particularly through import prices. But , although I hate to mention individual countries, I do want to point out that Irish inflation is also still under 2%.Question:I actually have two questions. The one concerns the economic outlook in the world and especially how would you see the trends right now in Russia, China and Brazil. And the other question concerns the sort of a code of conduct for ECB Board Members. The EU Commission announced yesterday that it will have a sort of a code of conduct for EU Commissioners, i.e. that all the Commissioners will have to reveal their economic connections and double salaries, etc. What about your members?Duisenberg:The first question. Mr. Noyer has just returned from the Far East. So he has the best insight into world economic developments. On the second question, I can be very brief: our code of conduct is in the Treaty. It is forbidden for Members of the Board to have any employment, whether paid or unpaid, outside their functions as Members of the Executive Board. That is what the Treaty says. But that relates to employment and therefore also to income from any employment. I know that, in the Finnish case, there has been some particular scrutiny of certain pension rights. They fall entirely outside this matter.Noyer:On the other question, I am sorry I have no particular new information on China or Brazil. On Brazil, there is no new information, at least not as far as we know. China\\'s situation might not be so clear: there are some new developments. But what is interesting in Asia is really that several countries that were hit by the first wave of difficulties, the ASEAN countries, are now moving upwards. If you take Malaysia, Thailand and South Korea, for instance, they are clearly countries in which reforms are being implemented and their economies seem to be gradually picking up. Perhaps this is less clear for other countries. It seems that, after all, there are good signs also coming from these regions, and from the emerging economies in general. So we still have to wait and see if this is confirmed, but there is not only bad news from an international point of view. May I also take this opportunity to say that it is clear that in these countries the rate of development in exports to European countries, as compared with the development of exports to the United States, has been roughly the same over the last two years. That is to say that, even if the US economy is seen to be more buoyant than that in Europe, one cannot say that the US economy has in fact helped these countries to recover more than the European economy, because the development of exports has been roughly the same over that period.With regard to Russia, I don\\'t think there are new developments that are obvious for the time being. They are clearly in the process of discussing with the IMF. The countries I was speaking of are proving to be in a better shape, i.e. countries that had negotiated a programme with the IMF some time ago and that have implemented that programme, so that first results of this policy are appearing. This is quite satisfactory.Question:What added complexity is there now in the setting of monetary policy, of interest rate policy, with the suggestion that the German economy shrunk in the last quarter of last year, whilst other major economies, of other countries, such as France, such as Holland and Spain, were still growing. Does this cause complications and additional difficulties in setting interest policy?Duisenberg:Well, it does make the evaluation and the judgement more difficult, undoubtedly because developments in various parts of Euroland seem to be diverging somewhat. So it causes us to say what is of the least complexity to reach what we have to, our euro area-wide view on developments.Question (translation):The Greek Minister of Finance stated in an interview on Sunday that, where the rate of inflation in Greece is concerned, where its accession to Monetary Union is concerned, this criterion might be weakened. In that context, he referred to both the Commission in Brussels and the European Central Bank. He implied that these suggestions had come from both institutions. Is he right?Duisenberg:We have, in the context of the General Council meeting I have just talked about, briefly discussed the issue without even trying to reach any conclusion. What is a fact is that in the future, when you have to judge a new entrant to the Euro zone, you will also have to make an assessment, as was done in the past, on the basis of the convergence criteria enumerated in the Treaty. Now, it is a factthat the Treaty was very precise in describing the criteria and how they had to be assessed in the first wave. The Treaty is a little less precise in describing exactly how to interpret the criteria in the case of additional countries joining a then already existing Euroland. For example, this particular Greek case, for which we have reached no conclusion. We will come back to that later on when - I am almost inclined to say when the time is right - when actual access to the euro area is about to be negotiated. Then we will come back to that. But - to mention just one example - one of the criteria was that the inflation rate should not deviate more than 1½ percentage points from the average rate of inflation of the three best-performing countries. Now, how do you interpret that in future, when you have an inflation rate for the euro area as a whole? Do you then look at the three best-performing countries? And, if so, do you look at eleven countries only or do you look at fifteen countries? Those are things to be discussed at the time when actual entry into the Euro zone becomes topical.Question:Yesterday, we heard some rumours in the markets that there are some interventions for the euro. Were those rumours or was it a fact?Duisenberg:It was just rumours. And they were not factual.Question (translation): A further question on the exchange rate. It has been said time and again that the exchange rate is also being \"talked down\" by politicians, not only by the economic data, by the spread between business activity and the interest rates, between Euroland and the United States. Mr. Stark, the Vice-President of the Bundesbank, has argued that someone in the political arena should say that a strong euro is favourable for the Monetary Union. Who should say that? Are you that someone or do you miss such a statement from the group of Finance Ministers?Duisenberg:I will say that a stable euro is good for the Monetary Union. And I will say that, most of all, in the proper forum, for example at the forthcoming meetings of the Euro 11 group. That is what we want.Question: In listening to your statement, it seems as though the risks came to the fore, and I would like to ask you if you think today that the Council has shifted its stance to looking more at the risks.Duisenberg:I don\\'t think so. Nor do I think that, if you compare today\\'s statement with last month\\'s, you will find a difference in tone. We still see risks, but we still see them both on the up side and on the down side.Question: Two questions. One, you mentioned the recent wage developments that there was an upside risk to inflation. Any word that you might have for the recent wage agreements found in Germany. Do they especially constitute such a risk? And the second question concerns the change in the auction style that you announced today for the longer term refinancing instruments. Is this change also an attempt to block so-called empty bids? Are you concerned about the high volume of bids that has been put in for the recent refinancing operations?Duisenberg:The first question on wage developments. Of course, the most recent ones we have become aware of were those in Germany. So, of course, in my statement we say that there is a potential risk involved. Not only a risk for prices. That, in particular, but also for growth and employment. So, of course, we have, in particular, those most recent wage developments in mind. Secondly, the phenomenon that the allotment percentage of the refinancing operations is very low and has even tended to become smaller, week after week, is a matter of concern. But the changeover for the longer-term refinancing operations has nothing to do with that. We will address this problem of a strong over-subscription to certain financing operations - which is there, in some countries more than in others - we will address those questions in the course of next weeks.Question:A few weeks ago you mentioned that the ECB might react to major misalignments in the exchange markets. Unfortunately, misalignment is open to different interpretations. So, would you please line out the main factors the ECB uses to detect misalignment?Duisenberg:I have said repeatedly that, within the broad range of indicators we monitor in order to determine our monetary policy strategy and decisions, the exchange rate is an important indicator, but only one. But it is an important one. And if we came to the judgement that the exchange rate developments are really seriously out of line with the economic fundamentals of the various large areas in the world - Europe and the United States - then we would probably react, but I cannot elaborate on that any more than I have done right now.Question (translation):Mr.Duisenberg, I have a question of a more general nature: How would you assess the European populace\\'s sentiment today, two months from the introduction of the euro? Do you feel that you have made good progress along the road of gradually gaining the trust of European citizens and where do you see a need to improve your communications to strengthen that confidence? And will English remain the only language in the long run in which the ECB wishes to communicate with the population in Europe?Duisenberg:I believe we are well on the way. What we also try to measure across Europe is the confidence that the population at large has in the euro. The results of those efforts to measure the degree of confidence are becoming ever more positive not only in the euro area, but even in some of the \"out\" countries and I could think of no means with which we could publish more than we are already doing and with which we could give more speeches than we are already delivering. I think we are making the most extensive publication efforts both in the form of articles or notes to be published, of speeches being given intended for the public at large to inspire confidence in the euro and I would not know what to do, where to find the time to do even more than that.As to the second part of the question, we do communicate in all languages. All our official publications, that are the Annual Report, the Quarterly Report, are published in the 11 languages of the community. We even decided, which was not compulsory, to publish our Monthly Bulletin in 11 languages in Europe, and then I must say we often give speeches in 11 languages, to the extent that we are able to do that.Question:Mr. President, do you see any signs at all in the euro zone of deflation, particularly in France and Germany?Duisenberg:We have carefully assessed that risk and we have come to the conclusion that there are no signs of deflation developing in the euro area as such at the moment.Question:Mr. Duisenberg, the German Government interprets the flat yield curve as an indicator for a shortage in the demand for capital, so that is one of the arguments behind the demands for lower interest rates. What is your opinion of this? Can it be interpreted that way?Duisenberg:We do not think so and we will also discuss that in a thorough manner in the context of the forthcoming Euro 11 meetings. But then I would like to add that the yield curve has risen somewhat and has already become somewhat steeper over the past few weeks.Question:I have two questions please: The Economic and Financial Committee - ex Monetary Committee - which met on Monday and Tuesday in Brussels seemed to have serious doubts about the ability of France and Germany to keep the budget deficit at around 2%. How much of this loosening of consolidation efforts is going into the weakening of the euro? Do you see that there is a lack of confidence which could lead to an increase in the longer term interest rates? That\\'s the first question. And then the second: Are you concerned that the weakening of the German economy could entail also a weakening of the Eastern European countries?Duisenberg:I am aware that the Economic and Financial Committee held a two-day meeting earlier this week in which they discussed the stability programs of France, Germany and a few other countries on the basis of an analysis by the European Commission which has in the meantime been made public. It contains some critical remarks with which the ECB fully agreed. And - as on earlier occasions - we, and now also the European Commission, have expressed some concern about the determination of, in particular, the larger countries to adhere to the aims of the Stability and Growth Pact and that in itself has led us to say that one of the risks we see of the upward risks is a looser fiscal position which you will find in my Introductory Statement, if you read it carefully again. The weakening of the German economy which has been evident over the last quarter of 1998 could, of course, have an impact on its surrounding partners, including - as you asked so specifically - Eastern Europe. And it is for that reason that we do hope and very much encourage the German Government to remedy some of the weakening of the economy, whilst at the same time emphasising that such a weakening of the economy is not being caused by nor can it be remedied by monetary policy measures.Question:Mr. Duisenberg, I would agree that today your outlook seems as if you might be looking for scope on the downside, maybe for an easing up as someone else just said, and that maybe prices are one of the key factors you are looking at right now?Duisenberg:But then you are \"hineininterpretierend\" in my statement in which I gave equal weight, I think, to the upward and the downward shifts which led to the conclusion not to change interest rates at all.Question: So you\\'re still neutral, you\\'re saying.Duisenberg:That is correct.Question:On foreign exchange, if I were a foreign exchange trader, you said that you want a stable euro. I almost think that we are not getting close to a euro-dollar level that makes you concerned right now. Is that a correct assumption?Duisenberg:Yes, I am not concerned with the level that the euro has currently reached. I already put it against the background of the levels of the D-Mark during last year and it is not so much the level that is of some concern. If the movement were to continue, there could be cause for concern.Question (translation):President Duisenberg, the Federal Minister of Finance, Mr. Lafontaine, was your guest. Did he call for a \"reduction of the key interest rates\" as emphatically as he is doing in public or did he \"mumble\" about it, and what did you tell him?Duisenberg:That was, yes, I believe that was two weeks ago, and I must confess that I have heard him louder through the media than in direct contact.Question:Mr. Duisenberg, would you share the concern of the German Government that a failure of European governments to reach an agreement on the financial architecture of the European Union by the end of March could cause turmoil on financial markets and would weaken the euro?Duisenberg:No, I am aware that one minister has made some statements to that effect today. I do not share that concern. I do not see what it can have to do with the functioning of the euro.Question:Mr. President, you said you talked to Mr. Lafontaine at the last meeting. He has given you a lot of suggestions what the ECB should be doing. You said that Germany maybe should take some steps to remedy the economy. Did you give him any specific advice about what Germany should do with its economy?Duisenberg:Not specifically to Germany, but we continuously give advice to point to the Ministers that the biggest problem they and we are facing in Europe is the unemployment problem. We keep explaining that the nature of that problem is predominantly of a structural nature, the lacking flexibility of markets, both labour and product markets, and that if governments can do anything they could tackle and, to our mind, they should tackle that cause of the unemployment in the first place.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rate_speeches_sent_tokenize():\n",
        "  '''\n",
        "  reload rate_speeches or update it with `sent_tokenize`\n",
        "  '''\n",
        "  global rate_speeches\n",
        "  RATE_SPEECHES_TOKENIZED_FILE = 'rate_speeches.sent_tokenize.parquet.gzip'\n",
        "\n",
        "  if os.path.exists(RATE_SPEECHES_TOKENIZED_FILE):\n",
        "    print(f\"loading {RATE_SPEECHES_TOKENIZED_FILE}...\")\n",
        "    rate_speeches = pd.read_parquet(RATE_SPEECHES_TOKENIZED_FILE)\n",
        "  else:\n",
        "    rate_speeches['extracted_text'] = rate_speeches['extracted_text'].apply(sent_tokenize)\n",
        "    rate_speeches.to_parquet(RATE_SPEECHES_TOKENIZED_FILE, compression='gzip')\n",
        "\n",
        "rate_speeches_sent_tokenize()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM5UZIR5oNHf",
        "outputId": "0ff8dd72-cfbd-4b40-83c5-f79cf890da3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading rate_speeches.sent_tokenize.parquet.gzip...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "from itertools import groupby\n",
        "from unicodedata import category as unicat\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.chunk import tree2conlltags\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.chunk.regexp import RegexpParser\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extract adverbial and adjective phrases, and transform\n",
        "    documents into lists of these keyphrases, with a total\n",
        "    keyphrase lexicon limited by the nfeatures parameter\n",
        "    and a document length limited/padded to doclen\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeatures: int, doclen: int):\n",
        "        self.grammar = r'KT: {(<RB.> <JJ.*>|<VB.*>|<RB.*>)|(<JJ> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {(<RB.*> <VB.>|<RB.>|<JJ.> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {<RB.>|<JJ.>}'\n",
        "        self.chunker = RegexpParser(self.grammar)\n",
        "        self.nfeatures = nfeatures\n",
        "        self.doclen = doclen\n",
        "        self._curr = 0\n",
        "\n",
        "    def normalize(self, sent):\n",
        "        \"\"\"\n",
        "        Removes punctuation from a tokenized/tagged sentence and\n",
        "        lowercases words.\n",
        "        \"\"\"\n",
        "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
        "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
        "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
        "        return list(sent)\n",
        "\n",
        "    def extract_candidate_phrases(self, sents, call_no):\n",
        "        \"\"\"\n",
        "        For a document, parse sentences using our chunker created by\n",
        "        our grammar, converting the parse tree into a tagged sequence.\n",
        "        Extract phrases, rejoin with a space, and yield the document\n",
        "        represented as a list of it's keyphrases.\n",
        "        \"\"\"\n",
        "        print(f\">> \\t [{call_no}] extract_candidate_phrases...\")\n",
        "\n",
        "        for i, sent in enumerate(sents):\n",
        "            tokens = word_tokenize(sent)\n",
        "            pos_tags = pos_tag(tokens)\n",
        "            normalized = self.normalize(pos_tags)\n",
        "            # print(f\"[{self._curr}] sent #{i}: {sent}\")\n",
        "            # print(f\"[{self._curr}] \\t {normalized}\")\n",
        "\n",
        "            chunks = tree2conlltags(self.chunker.parse(normalized))\n",
        "            if not chunks or all(chunk[-1] == 'O' for chunk in chunks):\n",
        "                #print(f\"No valid chunks found in sentence: {sent}\")\n",
        "                continue\n",
        "\n",
        "            phrases = [\n",
        "                \" \".join(word for word, pos, chunk in group).lower()\n",
        "                for key, group in groupby(\n",
        "                    chunks, lambda term: term[-1] != 'O'\n",
        "                ) if key\n",
        "            ]\n",
        "            for phrase in phrases:\n",
        "                yield phrase\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_lexicon(self, keydocs):\n",
        "        \"\"\"\n",
        "        Build a lexicon of size nfeatures\n",
        "        \"\"\"\n",
        "        keyphrases = [keyphrase for doc in keydocs for keyphrase in doc]\n",
        "        print(\"Keyphrases:\", keyphrases[:5])\n",
        "        fdist = FreqDist(keyphrases)\n",
        "        counts = fdist.most_common(self.nfeatures)\n",
        "        print(\"Frequency counts:\", counts[:5])\n",
        "        lexicon = [phrase for phrase, count in counts]\n",
        "        return {phrase: idx+1 for idx, phrase in enumerate(lexicon)}\n",
        "\n",
        "    def clip(self, keydoc, lexicon):\n",
        "        \"\"\"\n",
        "        Remove keyphrases from documents that aren't in the lexicon\n",
        "        \"\"\"\n",
        "        return [lexicon[keyphrase] for keyphrase in keydoc\n",
        "            if keyphrase in lexicon.keys()]\n",
        "\n",
        "    def transform(self, documents):\n",
        "      self._curr += 1\n",
        "      print(f\">> [{self._curr}] KeyphraseExtractor.transform: {len(documents)}...\")\n",
        "      docs = [list(self.extract_candidate_phrases(doc, i)) for i, doc in enumerate(documents)]\n",
        "      lexicon = self.get_lexicon(docs)\n",
        "      clipped = [list(self.clip(doc, lexicon)) for doc in docs]\n",
        "      return pad_sequences(clipped, maxlen=self.doclen)"
      ],
      "metadata": {
        "id": "T0tWYGJmIAIl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def data_set(view):\n",
        "  X = view['extracted_text']\n",
        "  display(X.describe())\n",
        "\n",
        "  # view = rate_speeches[:doclen]\n",
        "  labels = view[\"Direction\"].values\n",
        "  label_mapping = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
        "  encoded_labels = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "  # convert to one-hot encoding\n",
        "  num_classes = len(label_mapping)\n",
        "  print(f\"num_classes: {num_classes}\")\n",
        "  y = to_categorical(encoded_labels, num_classes=num_classes)\n",
        "  return X, y, num_classes\n"
      ],
      "metadata": {
        "id": "YHlL7BP1I6Cv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ7sWmqdhvqj",
        "outputId": "9c7ee588-873d-4280-b0b4-3fd3ee811e65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense, Input\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def create_lstm_model(vocab_size: int, input_length: int, num_classes: int):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_length,), name=\"input_layer\"),  # explicit input layer\n",
        "        Embedding(input_dim=vocab_size, output_dim=128, name=\"embedding_layer\"),\n",
        "        LSTM(128, return_sequences=True, name=\"lstm_layer_1\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_1\"),\n",
        "        LSTM(64, name=\"lstm_layer_2\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_2\"),\n",
        "        Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ghUGperddEGg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s_4cY_xi1ka",
        "outputId": "02965c47-4cf5-4eee-edfb-5038533bb615"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# X, y, num_classes = data_set(rate_speeches[:60])\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# keyphrase_extractor = KeyphraseExtractor(nfeatures=10000, doclen=100)\n",
        "# X_train = keyphrase_extractor.fit_transform(X_train)\n",
        "# X_val = keyphrase_extractor.transform(X_val)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v4_weIC7jd_W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = create_lstm_model(vocab_size=10000, input_length=100, num_classes=num_classes);\n",
        "\n",
        "# history = model.fit(\n",
        "#     X_train, y_train,\n",
        "#     validation_data=(X_val, y_val),\n",
        "#     epochs=10,\n",
        "#     batch_size=32\n",
        "# )\n"
      ],
      "metadata": {
        "id": "mTIihNKi44rx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loss, accuracy = model.evaluate(X_val, y_val)\n",
        "# print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "LeZ20D4OkxVT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DebugTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"Data Shape Before LSTM:\", X.shape)\n",
        "        # print(\"Sample Data Before LSTM:\", X[0])  # print a sample\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "Z9Oezl_Klq9B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from joblib import Memory\n",
        "\n",
        "X, y, num_classes = data_set(rate_speeches)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "0P8zESR_iKXf",
        "outputId": "3500025d-52e4-4e9e-a3be-8ca9075d5fec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "count                                                  3434\n",
              "unique                                                 3434\n",
              "top       [Willem F. Duisenberg, President of the Europe...\n",
              "freq                                                      1\n",
              "Name: extracted_text, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extracted_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[Willem F. Duisenberg, President of the Europe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set up caching directory\n",
        "memory = Memory(location=\"cache_directory\", verbose=0)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"keyphrase_extractor\", KeyphraseExtractor(nfeatures=10000, doclen=100)),\n",
        "    ('debug', DebugTransformer()),\n",
        "    (\"lstm_classifier\", KerasClassifier(\n",
        "        build_fn=create_lstm_model,\n",
        "        vocab_size=10000,\n",
        "        input_length=100,\n",
        "        num_classes=num_classes,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=1)\n",
        "    )\n",
        "], memory=memory)\n"
      ],
      "metadata": {
        "id": "Fsq4YdbqDWHR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = cross_val_score(pipeline, X, y, cv=4, scoring=\"accuracy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ht894d-0ESx2",
        "outputId": "1aaa906f-2807-47a7-9f4e-15ece13795cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> [1] KeyphraseExtractor.transform: 2575...\n",
            ">> \t [0] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [1] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [2] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [3] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [4] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [5] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [6] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [7] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [8] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [9] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [10] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [11] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [12] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [13] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [14] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [15] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [16] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [17] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [18] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [19] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [20] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [21] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [22] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [23] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [24] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [25] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [26] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [27] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [28] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [29] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [30] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [31] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [32] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [33] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [34] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [35] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [36] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [37] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [38] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [39] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [40] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [41] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [42] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [43] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [44] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [45] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [46] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [47] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [48] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [49] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [50] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [51] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [52] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [53] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [54] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [55] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [56] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [57] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [58] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [59] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [60] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [61] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [62] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [63] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [64] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [65] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [66] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [67] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [68] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [69] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [70] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [71] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [72] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [73] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [74] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [75] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [76] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [77] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [78] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [79] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [80] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [81] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [82] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [83] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [84] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [85] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [86] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [87] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [88] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [89] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [90] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [91] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [92] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [93] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [94] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [95] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [96] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [97] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [98] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [99] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [100] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [101] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [102] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [103] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [104] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [105] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [106] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [107] KeyphraseExtractor.extract_candidate_phrases\n",
            ">> \t [108] KeyphraseExtractor.extract_candidate_phrases\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9396792222e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \"\"\"\n\u001b[1;32m    468\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;31m# Return the output, without the metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshelving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# Returns the output but not the metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshelving\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, call_id, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         return self._after_call(call_id, args, kwargs, shelving,\n\u001b[1;32m    773\u001b[0m                                 output, start_time)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eedf6e1889eb>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> [{self._curr}] KeyphraseExtractor.transform: {len(documents)}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_candidate_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mclipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eedf6e1889eb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> [{self._curr}] KeyphraseExtractor.transform: {len(documents)}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_candidate_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mclipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-eedf6e1889eb>\u001b[0m in \u001b[0;36mextract_candidate_phrases\u001b[0;34m(self, sents, call_no)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# print(f\"[{self._curr}] \\t {normalized}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m#print(f\"No valid chunks found in sentence: {sent}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/chunk/util.py\u001b[0m in \u001b[0;36mtree2conlltags\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"B-\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                     raise ValueError(\n\u001b[1;32m    444\u001b[0m                         \u001b[0;34m\"Tree is too deeply nested to be printed in CoNLL format\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cross-validation scores: {scores}\")\n",
        "print(f\"Mean accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "tVVt3XcdJC_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b09ea4-c30d-4361-aafb-d9edf67b282f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.68 0.84 0.72 0.68]\n",
            "Mean accuracy: 0.73\n"
          ]
        }
      ]
    }
  ]
}