{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kubernetes-world/3666-ANLP-Predicting-ECB-Monetary-Policy/blob/main/LSTM-Final-SMOTE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diskcache\n",
        "!pip install boto3\n",
        "!pip install scikeras"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z_BjdltLgsjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmWJTYWx7Ibd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from itertools import groupby\n",
        "from unicodedata import category as unicat\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.chunk import tree2conlltags\n",
        "from nltk.chunk.regexp import RegexpParser\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from tensorflow.keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download cleaned data prepared in previous notebooks"
      ],
      "metadata": {
        "id": "l2z81pjkdD5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\"s3\", aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'), aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'))\n",
        "s3.download_file(\"3666-applied-nlp\", \"KeyphraseExtractor.cache.tar.gz\", \"KeyphraseExtractor.cache.tar.gz\")\n",
        "s3.download_file(\"3666-applied-nlp\", \"rate_speeches.sent_tokenize.parquet.gzip\", \"rate_speeches.sent_tokenize.parquet.gzip\")\n"
      ],
      "metadata": {
        "id": "zii75LcLG-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "RATE_SPEECHES_TOKENIZED_FILE = 'rate_speeches.sent_tokenize.parquet.gzip'\n",
        "assert os.path.exists(RATE_SPEECHES_TOKENIZED_FILE), f\"file not present: {RATE_SPEECHES_TOKENIZED_FILE}\"\n",
        "\n",
        "rate_speeches = pd.read_parquet(RATE_SPEECHES_TOKENIZED_FILE)\n",
        "print(f\"rate_speeches.shape: {rate_speeches.shape}\")\n"
      ],
      "metadata": {
        "id": "s4A9nVAndu3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf KeyphraseExtractor.cache.tar.gz"
      ],
      "metadata": {
        "id": "ruQ3VS-WTsWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from diskcache import Cache\n",
        "cache = Cache('KeyphraseExtractor.cache')\n"
      ],
      "metadata": {
        "id": "YzTjf9usTFrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `KeyphraseExtractor` transformer"
      ],
      "metadata": {
        "id": "sDUtjwI_bdJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from joblib import Memory\n",
        "# location = 'keyphrase_extractor_cachedir'\n",
        "# memory = Memory(location, verbose=0)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Extract adverbial and adjective phrases, and transform\n",
        "    documents into lists of these keyphrases, with a total\n",
        "    keyphrase lexicon limited by the nfeatures parameter\n",
        "    and a document length limited/padded to doclen\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeatures: int, doclen: int):\n",
        "        self.grammar = r'KT: {(<RB.> <JJ.*>|<VB.*>|<RB.*>)|(<JJ> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {(<RB.*> <VB.>|<RB.>|<JJ.> <NN.*>)}'\n",
        "        # self.grammar = r'KT: {<RB.>|<JJ.>}'\n",
        "        self.chunker = RegexpParser(self.grammar)\n",
        "        self.nfeatures = nfeatures\n",
        "        self.doclen = doclen\n",
        "        # self.normalize = memory.cache(self.normalize)\n",
        "        # self.extract_candidate_phrases = memory.cache(self.extract_candidate_phrases)\n",
        "\n",
        "    def normalize(self, sent):\n",
        "        \"\"\"\n",
        "        Removes punctuation from a tokenized/tagged sentence and\n",
        "        lowercases words.\n",
        "        \"\"\"\n",
        "        is_punct = lambda word: all(unicat(c).startswith('P') for c in word)\n",
        "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
        "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
        "        return list(sent)\n",
        "\n",
        "    # NOTE: wrap with memoizing function using cache\n",
        "    # repeated calls with the same arguments will lookup result in cache and avoid function evaluation.\n",
        "    @cache.memoize()\n",
        "    def extract_candidate_phrases(self, sents):\n",
        "        \"\"\"\n",
        "        For a document, parse sentences using our chunker created by\n",
        "        our grammar, converting the parse tree into a tagged sequence.\n",
        "        Extract phrases, rejoin with a space, and yield the document\n",
        "        represented as a list of it's keyphrases.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for sent in sents:\n",
        "            tokens = word_tokenize(sent)\n",
        "            pos_tags = pos_tag(tokens)\n",
        "            normalized = self.normalize(pos_tags)\n",
        "\n",
        "            chunks = tree2conlltags(self.chunker.parse(normalized))\n",
        "            if not chunks or all(chunk[-1] == 'O' for chunk in chunks):\n",
        "                # print(f\"No valid chunks found in sentence: {sent}\")\n",
        "                continue\n",
        "\n",
        "            phrases = [\n",
        "                \" \".join(word for word, pos, chunk in group).lower()\n",
        "                for key, group in groupby(\n",
        "                    chunks, lambda term: term[-1] != 'O'\n",
        "                ) if key\n",
        "            ]\n",
        "            for phrase in phrases:\n",
        "                # yield phrase # joblib.Memory can't cache generators\n",
        "                result.append(phrase)\n",
        "        return result\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_lexicon(self, keydocs):\n",
        "        \"\"\"\n",
        "        Build a lexicon of size nfeatures\n",
        "        \"\"\"\n",
        "        keyphrases = [keyphrase for doc in keydocs for keyphrase in doc]\n",
        "        # print(\"Keyphrases:\", keyphrases[:5])\n",
        "        fdist = FreqDist(keyphrases)\n",
        "        counts = fdist.most_common(self.nfeatures)\n",
        "        # print(\"Frequency counts:\", counts[:5])\n",
        "        lexicon = [phrase for phrase, count in counts]\n",
        "        return {phrase: idx+1 for idx, phrase in enumerate(lexicon)}\n",
        "\n",
        "    def clip(self, keydoc, lexicon):\n",
        "        \"\"\"\n",
        "        Remove keyphrases from documents that aren't in the lexicon\n",
        "        \"\"\"\n",
        "        return [lexicon[keyphrase] for keyphrase in keydoc if keyphrase in lexicon.keys()]\n",
        "\n",
        "    def transform(self, documents):\n",
        "      keydocs = [list(self.extract_candidate_phrases(doc)) for doc in tqdm(documents)]\n",
        "      lexicon = self.get_lexicon(keydocs)\n",
        "      clipped = [list(self.clip(keydoc, lexicon)) for keydoc in keydocs]\n",
        "      return pad_sequences(clipped, maxlen=self.doclen)\n"
      ],
      "metadata": {
        "id": "T0tWYGJmIAIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build `KeyphraseExtractor.cache` (skip)"
      ],
      "metadata": {
        "id": "ZHvD6wVKd-rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "# build cache\n",
        "keyphrase_extractor = KeyphraseExtractor(nfeatures=10000, doclen=100)\n",
        "keyphrase_extractor.fit_transform(X)\n",
        "\n",
        "# ---\n",
        "\n",
        "from google.colab import userdata\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\"s3\", aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'), aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'))\n",
        "s3.upload_file(\"KeyphraseExtractor.cache.tar.gz\", \"3666-applied-nlp\", \"KeyphraseExtractor.cache.tar.gz\")\n",
        "\n",
        "# ---\n",
        "\n",
        "!tar -cvzf KeyphraseExtractor.cache.tar.gz KeyphraseExtractor.cache\n"
      ],
      "metadata": {
        "id": "g5jj83Kmfby-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model"
      ],
      "metadata": {
        "id": "OKcwm_9Rbktn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense, Input\n",
        "\n",
        "def create_lstm_model(vocab_size: int, input_length: int, num_classes: int, compile_loss: str):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_length,), name=\"input_layer\"),  # explicit input layer\n",
        "        Embedding(input_dim=vocab_size, output_dim=128, name=\"embedding_layer\"),\n",
        "        LSTM(128, return_sequences=True, name=\"lstm_layer_1\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_1\"),\n",
        "        LSTM(64, name=\"lstm_layer_2\"),\n",
        "        Dropout(0.2, name=\"dropout_layer_2\"),\n",
        "        Dense(num_classes, activation=\"softmax\", name=\"output_layer\")\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss=compile_loss, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "0rsUrTZcSJAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DebugTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(f\"LSTM X.shape: {X.shape}\")\n",
        "        print(f\"Sample Data Before LSTM: {X[0]}\")  # print a sample\n",
        "        return X"
      ],
      "metadata": {
        "id": "A-eEwk6rfrqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label encoding and train/test splitting\n",
        "\n",
        "## No Data Leakage\n",
        "\n",
        "- It is acceptable to call `label_encoder.fit_transform(..)` on the entire set of labels (`rate_speeches[\"Direction\"]`) prior to splitting because `LabelEncoder` is a simple transformation that maps categorical values to numerical labels.\n",
        "\n",
        "- **It does not learn any information about the dataset beyond the unique classes.**\n"
      ],
      "metadata": {
        "id": "MX0RV7g2giNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = rate_speeches[\"extracted_text\"]\n",
        "print(f\"X.shape: {X.shape}\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(rate_speeches[\"Direction\"])  # integer encoding\n",
        "print(f\"y_encoded.shape: {y_encoded.shape}\")\n"
      ],
      "metadata": {
        "id": "pImP292Lgjow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "vR28e5e-a0rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `StratifiedKFold` (and choice of `n_splits`)\n",
        "\n",
        "## 5-Fold Cross-Validation:\n",
        "- Most commonly used.\n",
        "- Provides a good balance between bias and variance of the performance estimate.\n",
        "\n",
        "## 10-Fold Cross-Validation:\n",
        "- Preferred when data is limited and computational cost is acceptable.\n",
        "- Offers slightly more robust performance estimates but increases computation time."
      ],
      "metadata": {
        "id": "uc8uSOcwmX1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# dynamically determine n_splits based on class distribution\n",
        "min_class_size = rate_speeches[\"Direction\"].value_counts().min()\n",
        "print(f\"min_class_size: {min_class_size}\")\n",
        "\n",
        "n_splits = min(10, min_class_size)  # use up to 10 splits or the smallest class size\n",
        "print(f\"Using {n_splits}-Fold Cross-Validation\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "ub3UMSYFZJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard `sklearn.pipeline.Pipeline`"
      ],
      "metadata": {
        "id": "XjEgWBmHjs5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from joblib import Memory\n",
        "\n",
        "#-- no longer needed since the results from KeyphraseExtractor have already been cached\n",
        "# memory_sparse = Memory(location=\"cache_directory_sparse\", verbose=0)\n",
        "\n",
        "pipeline_scce = Pipeline([\n",
        "    (\"keyphrase_extractor\",\n",
        "      KeyphraseExtractor(nfeatures=10000, doclen=100)),\n",
        "    ('debug', DebugTransformer()),\n",
        "    (\"lstm_classifier\",\n",
        "      KerasClassifier(\n",
        "        build_fn=create_lstm_model,\n",
        "        vocab_size=10000,\n",
        "        input_length=100,\n",
        "        num_classes=3,\n",
        "        compile_loss='sparse_categorical_crossentropy',\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=1)\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "IcGqYzxxeHO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYDdKRsfYQLk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = cross_val_score(pipeline_scce, X_train, y_train, cv=skf, scoring=\"accuracy\")\n"
      ],
      "metadata": {
        "id": "b5rju1pFSJg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cross-validation scores: {scores}\")\n",
        "print(f\"Mean accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "aamEA-X7addX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Final Evaluation on Test Set\n",
        "pipeline_scce.fit(X_train, y_train)\n",
        "y_pred = pipeline_scce.predict(X_test)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ctiZarR-vp3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred, labels=range(len(label_encoder.classes_)))\n",
        "print(cm)\n",
        "\n",
        "print()\n",
        "# display Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "KlmjUFGRyJDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Minority Over-sampling TEchnique (`SMOTE`) for imbalanced data\n",
        "\n",
        "## Cross-Validation:\n",
        "\n",
        "- Perform cross-validation with a pipeline to ensure that `SMOTE` is applied only on the training data in each fold.\n",
        "\n",
        "## Use `imblearn.pipeline.Pipeline`:\n",
        "\n",
        "- The `Pipeline` class in `imblearn` is specifically designed for scenarios where the pipeline modifies both X and y, such as oversampling with `SMOTE`.\n",
        "\n",
        "## The `SMOTE` pipeline step:\n",
        "\n",
        "- We place SMOTE after the `KeyphraseExtractor` preprocessing steps (as we would do with scaling or encoding) to ensure `SMOTE` operates on the transformed data.\n",
        "\n",
        "## Avoiding data leakage:\n",
        "\n",
        "- When using `Pipeline` with `SMOTE`, oversampling is applied only to the training folds during cross-validation.\n",
        "- This avoids data leakage into the validation set, which would inflate performance metrics.\n"
      ],
      "metadata": {
        "id": "4Ck7vpnMjy0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbalancedPipeline\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "imbalanced_pipeline = ImbalancedPipeline(steps=[\n",
        "    (\"keyphrase_extractor\",\n",
        "      KeyphraseExtractor(nfeatures=10000, doclen=100)),\n",
        "    (\"smote\",\n",
        "      SMOTE(random_state=42)),\n",
        "    ('debug', DebugTransformer()),\n",
        "    (\"lstm_classifier\",\n",
        "      KerasClassifier(\n",
        "        build_fn=create_lstm_model,\n",
        "        vocab_size=10000,\n",
        "        input_length=100,\n",
        "        num_classes=3,\n",
        "        compile_loss='sparse_categorical_crossentropy',\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=1))\n",
        "])\n",
        "\n",
        "scores = cross_validate(\n",
        "    imbalanced_pipeline,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=skf,\n",
        "    scoring=\"accuracy\",\n",
        "    return_estimator=True,\n",
        "    return_train_score=True)\n"
      ],
      "metadata": {
        "id": "W85GQ_XuNbz8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(scores)\n",
        "print(f\"Cross-validation scores: {scores['test_score']}\")\n",
        "print(f\"Mean accuracy: {np.mean(scores['test_score']):.2f}\")\n"
      ],
      "metadata": {
        "id": "-C_Q5olwTzBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final evaluation on Test data set (from split)"
      ],
      "metadata": {
        "id": "LduABfa-aNVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "imbalanced_pipeline.fit(X_train, y_train)\n",
        "y_pred = imbalanced_pipeline.predict(X_test)\n"
      ],
      "metadata": {
        "id": "6ofITW8nUnjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred, labels=range(len(label_encoder.classes_)))\n",
        "print(cm)\n",
        "\n",
        "print()\n",
        "# display Confusion Matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "XkRd2hAHVC68"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}